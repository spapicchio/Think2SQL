[project]
name = "think2sql"
version = "0.1.0"
authors = [
    { name = "Simone Papicchio", email = "simone.papicchio@polito.it" },
    { name = "Simone Rossi", email = "simone.rossi.93@gmail.com" },
]
description = "Official implementation of Think2SQL"
# readme = "README.md"
requires-python = ">=3.12,<3.13"  # vllm needs Python <3.13 with dev
dependencies = [
    "deepspeed>=0.16.8",
    "deepspeed-kernels>=0.0.1.dev1698255861",
    "vllm==0.10.2",
    "ray",
    "accelerate>=1.4.0",
    "transformers>=4.56.0",
    "trl>=0.23.0",
    "more_itertools",
    "datasets>=4.0.0",
    "hf-transfer>=0.1.9",
    "jinja2>=3.1.6",
    "loguru>=0.7.3",
    "nl2sqlevaluator>=1.3.2",
    "python-dotenv>=1.1.1",
    "sentencepiece>=0.2.1",
    "wandb>=0.21.3",
    "bitsandbytes>=0.47.0",
    "fire>=0.7.1",
    "matplotlib>=3.10.6",
    "emoji>=2.15.0",
    "pytest>=8.4.2",
    "notebook>=7.4.7",
    "litellm>=1.77.7",
    "flashinfer-python>=0.4.1",
    "flashinfer-cubin>=0.4.1",
    "flash-attn==2.8.3",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"


[tool.uv.sources]
flashinfer-jit-cache = { index = "flashinfer" }
# https://github.com/Dao-AILab/flash-attention/releases/ if you change python version or cuda version or torch version you have to change also the whell
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiFALSE-cp312-cp312-linux_x86_64.whl" }

[[tool.uv.index]]
name = "flashinfer"
url = "https://flashinfer.ai/whl/cu124"